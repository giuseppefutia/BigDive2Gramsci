{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rgcn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuseppefutia/BigDive2Gramsci/blob/master/rgcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hUJCrXRIj0zN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A step-by-step guide to build R-GCN in Pytorch"
      ]
    },
    {
      "metadata": {
        "id": "RSe5Pmn6pbdu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook I describe how to develop Relational Graph Convolutional Networks (R-GCNs) of the link-prediction task within Knowledge Graphs. For more information on R-GCNs we suggest the following article:\n",
        "\n",
        "\n",
        "*   Schlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., & Welling, M. (2018, June). [Modeling relational data with graph convolutional networks](https://link.springer.com/chapter/10.1007/978-3-319-93417-4_38). In European Semantic Web Conference (pp. 593-607). Springer, Cham.\n",
        "\n",
        "I take some parts of this article in order to describe and explain the source code deployed in this notebook. The main part of this code is inspired by the Deep Graph Library (DGL - https://www.dgl.ai/), that is a python package built to ease deep learning on graph, on top of existing deep learning frameworks. \n",
        "\n",
        "## A little vocabulary\n",
        "\n",
        "### Knowledge graphs\n",
        "TODO\n",
        "\n",
        "### Link prediction\n",
        "TO DO: DESCRIBE HOW THE LINK PREDICTION WORKS.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rjGtOjY8cD73",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install required libraries"
      ]
    },
    {
      "metadata": {
        "id": "WfbL9l93cI49",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install dgl\n",
        "!pip install rdflib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eYO-zCwXgw1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ]
    },
    {
      "metadata": {
        "id": "vjAD4fq_cck8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available(): # Try to use GPU if available\n",
        "  device = torch.device('cuda')\n",
        "\n",
        "print('Your device is ' + str(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qgkie2ZaOemx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ]
    },
    {
      "metadata": {
        "id": "QceuLDaMSwti",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Most code is adapted from authors' implementation of RGCN link prediction:\n",
        "https://github.com/MichSchli/RelationPrediction."
      ]
    },
    {
      "metadata": {
        "id": "NFXMJbFEO0Wm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions for training and testing graphs"
      ]
    },
    {
      "metadata": {
        "id": "Bz6DvX1qOeM1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_adj_and_degrees(num_nodes, triplets):\n",
        "    \"\"\" Get adjacency list and degrees of the graph\n",
        "    \"\"\"\n",
        "    adj_list = [[] for _ in range(num_nodes)]\n",
        "    for i,triplet in enumerate(triplets):\n",
        "        adj_list[triplet[0]].append([i, triplet[2]])\n",
        "        adj_list[triplet[2]].append([i, triplet[0]])\n",
        "\n",
        "    degrees = np.array([len(a) for a in adj_list])\n",
        "    adj_list = [np.array(a) for a in adj_list]\n",
        "    return adj_list, degrees\n",
        "\n",
        "def sample_edge_neighborhood(adj_list, degrees, n_triplets, sample_size):\n",
        "    \"\"\" Edge neighborhood sampling to reduce training graph size\n",
        "    \"\"\"\n",
        "\n",
        "    edges = np.zeros((sample_size), dtype=np.int32)\n",
        "\n",
        "    #initialize\n",
        "    sample_counts = np.array([d for d in degrees])\n",
        "    picked = np.array([False for _ in range(n_triplets)])\n",
        "    seen = np.array([False for _ in degrees])\n",
        "\n",
        "    for i in range(0, sample_size):\n",
        "        weights = sample_counts * seen\n",
        "\n",
        "        if np.sum(weights) == 0:\n",
        "            weights = np.ones_like(weights)\n",
        "            weights[np.where(sample_counts == 0)] = 0\n",
        "\n",
        "        probabilities = (weights) / np.sum(weights)\n",
        "        chosen_vertex = np.random.choice(np.arange(degrees.shape[0]),\n",
        "                                         p=probabilities)\n",
        "        chosen_adj_list = adj_list[chosen_vertex]\n",
        "        seen[chosen_vertex] = True\n",
        "\n",
        "        chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
        "        chosen_edge = chosen_adj_list[chosen_edge]\n",
        "        edge_number = chosen_edge[0]\n",
        "\n",
        "        while picked[edge_number]:\n",
        "            chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
        "            chosen_edge = chosen_adj_list[chosen_edge]\n",
        "            edge_number = chosen_edge[0]\n",
        "\n",
        "        edges[i] = edge_number\n",
        "        other_vertex = chosen_edge[1]\n",
        "        picked[edge_number] = True\n",
        "        sample_counts[chosen_vertex] -= 1\n",
        "        sample_counts[other_vertex] -= 1\n",
        "        seen[other_vertex] = True\n",
        "\n",
        "    return edges\n",
        "\n",
        "def generate_sampled_graph_and_labels(triplets, sample_size, split_size,\n",
        "                                      num_rels, adj_list, degrees,\n",
        "                                      negative_rate):\n",
        "    \"\"\"Get training graph and signals\n",
        "    First perform edge neighborhood sampling on graph, then perform negative\n",
        "    sampling to generate negative samples\n",
        "    \"\"\"\n",
        "    # perform edge neighbor sampling\n",
        "    edges = sample_edge_neighborhood(adj_list, degrees, len(triplets),\n",
        "                                     sample_size)\n",
        "\n",
        "    # relabel nodes to have consecutive node ids\n",
        "    edges = triplets[edges]\n",
        "    src, rel, dst = edges.transpose()\n",
        "    uniq_v, edges = np.unique((src, dst), return_inverse=True)\n",
        "    src, dst = np.reshape(edges, (2, -1))\n",
        "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
        "\n",
        "    # negative sampling\n",
        "    samples, labels = negative_sampling(relabeled_edges, len(uniq_v),\n",
        "                                        negative_rate)\n",
        "\n",
        "    # further split graph, only half of the edges will be used as graph\n",
        "    # structure, while the rest half is used as unseen positive samples\n",
        "    split_size = int(sample_size * split_size)\n",
        "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
        "                                       size=split_size, replace=False)\n",
        "    src = src[graph_split_ids]\n",
        "    dst = dst[graph_split_ids]\n",
        "    rel = rel[graph_split_ids]\n",
        "\n",
        "    # build DGL graph\n",
        "    print(\"# sampled nodes: {}\".format(len(uniq_v)))\n",
        "    print(\"# sampled edges: {}\".format(len(src) * 2))\n",
        "    g, rel, norm = build_graph_from_triplets(len(uniq_v), num_rels,\n",
        "                                             (src, rel, dst))\n",
        "    return g, uniq_v, rel, norm, samples, labels\n",
        "\n",
        "def comp_deg_norm(g):\n",
        "    in_deg = g.in_degrees(range(g.number_of_nodes())).float().numpy()\n",
        "    norm = 1.0 / in_deg\n",
        "    norm[np.isinf(norm)] = 0\n",
        "    return norm\n",
        "\n",
        "def build_graph_from_triplets(num_nodes, num_rels, triplets):\n",
        "    \"\"\" Create a DGL graph. The graph is bidirectional because RGCN authors\n",
        "        use reversed relations.\n",
        "        This function also generates edge type and normalization factor\n",
        "        (reciprocal of node incoming degree)\n",
        "    \"\"\"\n",
        "    g = dgl.DGLGraph()\n",
        "    g.add_nodes(num_nodes)\n",
        "    src, rel, dst = triplets\n",
        "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
        "    rel = np.concatenate((rel, rel + num_rels))\n",
        "    edges = sorted(zip(dst, src, rel))\n",
        "    dst, src, rel = np.array(edges).transpose()\n",
        "    g.add_edges(src, dst)\n",
        "    norm = comp_deg_norm(g)\n",
        "    print(\"# nodes: {}, # edges: {}\".format(num_nodes, len(src)))\n",
        "    return g, rel, norm\n",
        "\n",
        "def build_test_graph(num_nodes, num_rels, edges):\n",
        "    src, rel, dst = edges.transpose()\n",
        "    print(\"Test graph:\")\n",
        "    return build_graph_from_triplets(num_nodes, num_rels, (src, rel, dst))\n",
        "\n",
        "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
        "    size_of_batch = len(pos_samples)\n",
        "    num_to_generate = size_of_batch * negative_rate\n",
        "    neg_samples = np.tile(pos_samples, (negative_rate, 1))\n",
        "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
        "    labels[: size_of_batch] = 1\n",
        "    values = np.random.randint(num_entity, size=num_to_generate)\n",
        "    choices = np.random.uniform(size=num_to_generate)\n",
        "    subj = choices > 0.5\n",
        "    obj = choices <= 0.5\n",
        "    neg_samples[subj, 0] = values[subj]\n",
        "    neg_samples[obj, 2] = values[obj]\n",
        "\n",
        "    return np.concatenate((pos_samples, neg_samples)), labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1FG_eBC4Phk8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions for evaluation"
      ]
    },
    {
      "metadata": {
        "id": "2hWSpJTKPiBG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sort_and_rank(score, target):\n",
        "    _, indices = torch.sort(score, dim=1, descending=True)\n",
        "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
        "    indices = indices[:, 1].view(-1)\n",
        "    return indices\n",
        "\n",
        "def perturb_and_get_rank(embedding, w, a, r, b, num_entity, batch_size=100):\n",
        "    \"\"\" Perturb one element in the triplets\n",
        "    \"\"\"\n",
        "    n_batch = (num_entity + batch_size - 1) // batch_size\n",
        "    ranks = []\n",
        "    for idx in range(n_batch):\n",
        "        print(\"batch {} / {}\".format(idx, n_batch))\n",
        "        batch_start = idx * batch_size\n",
        "        batch_end = min(num_entity, (idx + 1) * batch_size)\n",
        "        batch_a = a[batch_start: batch_end]\n",
        "        batch_r = r[batch_start: batch_end]\n",
        "        emb_ar = embedding[batch_a] * w[batch_r]\n",
        "        emb_ar = emb_ar.transpose(0, 1).unsqueeze(2) # size: D x E x 1\n",
        "        emb_c = embedding.transpose(0, 1).unsqueeze(1) # size: D x 1 x V\n",
        "        # out-prod and reduce sum\n",
        "        out_prod = torch.bmm(emb_ar, emb_c) # size D x E x V\n",
        "        score = torch.sum(out_prod, dim=0) # size E x V\n",
        "        score = torch.sigmoid(score)\n",
        "        target = b[batch_start: batch_end]\n",
        "        ranks.append(sort_and_rank(score, target))\n",
        "    return torch.cat(ranks)\n",
        "\n",
        "\n",
        "# return MRR (raw), and Hits @ (1, 3, 10)\n",
        "def evaluate(test_graph, model, test_triplets, num_entity, hits=[], eval_bz=100):\n",
        "    with torch.no_grad():\n",
        "        embedding, w = model.evaluate(test_graph)\n",
        "        s = test_triplets[:, 0]\n",
        "        r = test_triplets[:, 1]\n",
        "        o = test_triplets[:, 2]\n",
        "\n",
        "        # perturb subject\n",
        "        ranks_s = perturb_and_get_rank(embedding, w, o, r, s, num_entity, eval_bz)\n",
        "        # perturb object\n",
        "        ranks_o = perturb_and_get_rank(embedding, w, s, r, o, num_entity, eval_bz)\n",
        "\n",
        "        ranks = torch.cat([ranks_s, ranks_o])\n",
        "        ranks += 1 # change to 1-indexed\n",
        "\n",
        "        mrr = torch.mean(1.0 / ranks.float())\n",
        "        print(\"MRR (raw): {:.6f}\".format(mrr.item()))\n",
        "\n",
        "        for hit in hits:\n",
        "            avg_count = torch.mean((ranks <= hit).float())\n",
        "            print(\"Hits (raw) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
        "    return mrr.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_11LN9--BVUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# R-GCN Layers"
      ]
    },
    {
      "metadata": {
        "id": "MC5lPUdxBagG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I describe the implementation of a R-GCN Layer, starting from an abstract class. "
      ]
    },
    {
      "metadata": {
        "id": "fLwWlaReB8kv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RGCNLayer(nn.Module):\n",
        "  '''\n",
        "  The abstract layer of a R-GCN is built starting from the following parameters:\n",
        "  in_feat --  \n",
        "  out_feat --\n",
        "  bias --\n",
        "  activation --\n",
        "  self_loop --\n",
        "  dropout --\n",
        "  '''\n",
        "  def __init__(self, in_feat, out_feat, bias=None, activation=None,\n",
        "                 self_loop=False, dropout=0.0):\n",
        "    \n",
        "    super(RGCNLayer, self).__init__()\n",
        "        \n",
        "    self.bias = bias\n",
        "    self.activation = activation\n",
        "    self.self_loop = self_loop\n",
        "\n",
        "    if self.bias == True:\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
        "        nn.init.xavier_uniform_(self.bias,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    # weight for self loop\n",
        "    if self.self_loop:\n",
        "        self.loop_weight = nn.Parameter(torch.Tensor(in_feat, out_feat))\n",
        "        nn.init.xavier_uniform_(self.loop_weight,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    if dropout:\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    else:\n",
        "        self.dropout = None\n",
        "\n",
        "  # define how propagation is done in subclass\n",
        "  def propagate(self, g):\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def forward(self, g):\n",
        "      if self.self_loop:\n",
        "          loop_message = torch.mm(g.ndata['h'], self.loop_weight)\n",
        "          if self.dropout is not None:\n",
        "              loop_message = self.dropout(loop_message)\n",
        "\n",
        "      self.propagate(g)\n",
        "\n",
        "      # apply bias and activation\n",
        "      node_repr = g.ndata['h']\n",
        "      if self.bias:\n",
        "          node_repr = node_repr + self.bias\n",
        "      if self.self_loop:\n",
        "          node_repr = node_repr + loop_message\n",
        "      if self.activation:\n",
        "          node_repr = self.activation(node_repr)\n",
        "\n",
        "      g.ndata['h'] = node_repr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-ue0D5pG0nj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tips**:\n",
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UtPxe53-LfWs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Basis Layer"
      ]
    },
    {
      "metadata": {
        "id": "gx7EA7LdLmRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RGCNBasisLayer(RGCNLayer):\n",
        "    \n",
        "    def __init__(self, in_feat, out_feat, num_rels, num_bases=-1, bias=None,\n",
        "                 activation=None, is_input_layer=False):\n",
        "        \n",
        "        super(RGCNBasisLayer, self).__init__(in_feat, out_feat, bias, activation)\n",
        "        \n",
        "        self.in_feat = in_feat\n",
        "        self.out_feat = out_feat\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        self.is_input_layer = is_input_layer\n",
        "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
        "            self.num_bases = self.num_rels\n",
        "\n",
        "        # add basis weights\n",
        "        self.weight = nn.Parameter(torch.Tensor(self.num_bases, self.in_feat,\n",
        "                                                self.out_feat))\n",
        "        if self.num_bases < self.num_rels:\n",
        "            # linear combination coefficients\n",
        "            self.w_comp = nn.Parameter(torch.Tensor(self.num_rels,\n",
        "                                                    self.num_bases))\n",
        "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
        "        if self.num_bases < self.num_rels:\n",
        "            nn.init.xavier_uniform_(self.w_comp,\n",
        "                                    gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def propagate(self, g):\n",
        "        if self.num_bases < self.num_rels:\n",
        "            # generate all weights from bases\n",
        "            weight = self.weight.view(self.num_bases,\n",
        "                                      self.in_feat * self.out_feat)\n",
        "            weight = torch.matmul(self.w_comp, weight).view(\n",
        "                                    self.num_rels, self.in_feat, self.out_feat)\n",
        "        else:\n",
        "            weight = self.weight\n",
        "\n",
        "        if self.is_input_layer:\n",
        "            def msg_func(edges):\n",
        "                # for input layer, matrix multiply can be converted to be\n",
        "                # an embedding lookup using source node id\n",
        "                embed = weight.view(-1, self.out_feat)\n",
        "                index = edges.data['type'] * self.in_feat + edges.src['id']\n",
        "                return {'msg': embed[index] * edges.data['norm']}\n",
        "        else:\n",
        "            def msg_func(edges):\n",
        "                w = weight[edges.data['type']]\n",
        "                msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
        "                msg = msg * edges.data['norm']\n",
        "                return {'msg': msg}\n",
        "\n",
        "        g.update_all(msg_func, fn.sum(msg='msg', out='h'), None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GavDRPWgNd3Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tips**\n",
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6JXGXndBLiiE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ## Block Layer"
      ]
    },
    {
      "metadata": {
        "id": "RB8ePcXwMP15",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RGCNBlockLayer(RGCNLayer):\n",
        "    \n",
        "    def __init__(self, in_feat, out_feat, num_rels, num_bases, bias=None,\n",
        "                 activation=None, self_loop=False, dropout=0.0):\n",
        "        \n",
        "        super(RGCNBlockLayer, self).__init__(in_feat, out_feat, bias,\n",
        "                                             activation, self_loop=self_loop,\n",
        "                                             dropout=dropout)\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        assert self.num_bases > 0\n",
        "\n",
        "        self.out_feat = out_feat\n",
        "        self.submat_in = in_feat // self.num_bases\n",
        "        self.submat_out = out_feat // self.num_bases\n",
        "\n",
        "        # assuming in_feat and out_feat are both divisible by num_bases\n",
        "        self.weight = nn.Parameter(torch.Tensor(\n",
        "            self.num_rels, self.num_bases * self.submat_in * self.submat_out))\n",
        "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def msg_func(self, edges):\n",
        "        weight = self.weight[edges.data['type']].view(\n",
        "                    -1, self.submat_in, self.submat_out)\n",
        "        node = edges.src['h'].view(-1, 1, self.submat_in)\n",
        "        msg = torch.bmm(node, weight).view(-1, self.out_feat)\n",
        "        return {'msg': msg}\n",
        "\n",
        "    def propagate(self, g):\n",
        "        g.update_all(self.msg_func, fn.sum(msg='msg', out='h'), self.apply_func)\n",
        "\n",
        "    def apply_func(self, nodes):\n",
        "        return {'h': nodes.data['h'] * nodes.data['norm']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EwIBPm4YNkcb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tips**\n",
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EIgGxaxSRZRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# R-GCN Model"
      ]
    },
    {
      "metadata": {
        "id": "smN7wF3sRgk3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BaseRGCN(nn.Module):\n",
        "    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases=-1,\n",
        "                 num_hidden_layers=1, dropout=0, use_cuda=False):\n",
        "        super(BaseRGCN, self).__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.h_dim = h_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        # create rgcn layers\n",
        "        self.build_model()\n",
        "\n",
        "        # create initial features\n",
        "        self.features = self.create_features()\n",
        "\n",
        "    def build_model(self):\n",
        "        self.layers = nn.ModuleList()\n",
        "        # i2h\n",
        "        i2h = self.build_input_layer()\n",
        "        if i2h is not None:\n",
        "            self.layers.append(i2h)\n",
        "        # h2h\n",
        "        for idx in range(self.num_hidden_layers):\n",
        "            h2h = self.build_hidden_layer(idx)\n",
        "            self.layers.append(h2h)\n",
        "        # h2o\n",
        "        h2o = self.build_output_layer()\n",
        "        if h2o is not None:\n",
        "            self.layers.append(h2o)\n",
        "\n",
        "    # initialize feature for each node\n",
        "    def create_features(self):\n",
        "        return None\n",
        "\n",
        "    def build_input_layer(self):\n",
        "        return None\n",
        "\n",
        "    def build_hidden_layer(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build_output_layer(self):\n",
        "        return None\n",
        "\n",
        "    def forward(self, g):\n",
        "        if self.features is not None:\n",
        "            g.ndata['id'] = self.features\n",
        "        for layer in self.layers:\n",
        "            layer(g)\n",
        "        return g.ndata.pop('h')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CWvu7NIgS5OE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Link prediction stage"
      ]
    },
    {
      "metadata": {
        "id": "E2ciIAQFS8Z6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "from dgl.contrib.data import load_data\n",
        "\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, num_nodes, h_dim):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(num_nodes, h_dim)\n",
        "\n",
        "    def forward(self, g):\n",
        "        node_id = g.ndata['id'].squeeze()\n",
        "        g.ndata['h'] = self.embedding(node_id)\n",
        "\n",
        "class RGCN(BaseRGCN):\n",
        "    def build_input_layer(self):\n",
        "        return EmbeddingLayer(self.num_nodes, self.h_dim)\n",
        "\n",
        "    def build_hidden_layer(self, idx):\n",
        "        act = F.relu if idx < self.num_hidden_layers - 1 else None\n",
        "        \n",
        "        return RGCNBlockLayer(self.h_dim, self.h_dim, self.num_rels, self.num_bases,\n",
        "                         activation=act, self_loop=True, dropout=self.dropout)\n",
        "\n",
        "class LinkPredict(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, num_rels, num_bases=-1,\n",
        "                 num_hidden_layers=1, dropout=0, use_cuda=False, reg_param=0):\n",
        "        super(LinkPredict, self).__init__()\n",
        "        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, num_bases,\n",
        "                         num_hidden_layers, dropout, use_cuda)\n",
        "        self.reg_param = reg_param\n",
        "        self.w_relation = nn.Parameter(torch.Tensor(num_rels, h_dim))\n",
        "        nn.init.xavier_uniform_(self.w_relation,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def calc_score(self, embedding, triplets):\n",
        "        # DistMult\n",
        "        s = embedding[triplets[:,0]]\n",
        "        r = self.w_relation[triplets[:,1]]\n",
        "        o = embedding[triplets[:,2]]\n",
        "        score = torch.sum(s * r * o, dim=1)\n",
        "        return score\n",
        "\n",
        "    def forward(self, g):\n",
        "        return self.rgcn.forward(g)\n",
        "\n",
        "    def evaluate(self, g):\n",
        "        # get embedding and relation weight without grad\n",
        "        embedding = self.forward(g)\n",
        "        return embedding, self.w_relation\n",
        "\n",
        "    def regularization_loss(self, embedding):\n",
        "        return torch.mean(embedding.pow(2)) + torch.mean(self.w_relation.pow(2))\n",
        "\n",
        "    def get_loss(self, g, triplets, labels):\n",
        "        # triplets is a list of data samples (positive and negative)\n",
        "        # each row in the triplets is a 3-tuple of (source, relation, destination)\n",
        "        embedding = self.forward(g)\n",
        "        score = self.calc_score(embedding, triplets)\n",
        "        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n",
        "        reg_loss = self.regularization_loss(embedding)\n",
        "        return predict_loss + self.reg_param * reg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PWh7JG1YZRqW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ]
    },
    {
      "metadata": {
        "id": "sfuN9kevZ7Lx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args = {'dropout': 0.2,\n",
        "        'n_hidden': 500,\n",
        "        'gpu': 1,\n",
        "        'lr': 1e-2,\n",
        "        'n_bases': 100,\n",
        "        'n_layers': 2,\n",
        "        'n_epochs': 6000,\n",
        "        'dataset': 'FB15k-237',\n",
        "        'eval_batch_size': 500,\n",
        "        'regularization': 0.01,\n",
        "        'grad_norm': 1.0,\n",
        "        'graph_batch_size': 30000,\n",
        "        'graph_split_size': 0.5,\n",
        "        'negative_sample': 10,\n",
        "        'evaluate_every': 500}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wi7zJ-75Tme_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Scripts for running"
      ]
    },
    {
      "metadata": {
        "id": "lH9q4lKuTojg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load graph data\n",
        "data = load_data(args['dataset'])\n",
        "num_nodes = data.num_nodes\n",
        "train_data = data.train\n",
        "valid_data = data.valid\n",
        "test_data = data.test\n",
        "num_rels = data.num_rels\n",
        "\n",
        "\n",
        "# create model\n",
        "model = LinkPredict(num_nodes,\n",
        "                    args['n_hidden'],\n",
        "                    num_rels,\n",
        "                    num_bases=args['n_bases'],\n",
        "                    num_hidden_layers=args['n_layers'],\n",
        "                    dropout=args['dropout'],\n",
        "                    use_cuda=use_cuda,\n",
        "                    reg_param=args['regularization'])\n",
        "\n",
        "# validation and testing triplets\n",
        "valid_data = torch.LongTensor(valid_data)\n",
        "test_data = torch.LongTensor(test_data)\n",
        "\n",
        "# build test graph\n",
        "test_graph, test_rel, test_norm = build_test_graph(\n",
        "    num_nodes, num_rels, train_data)\n",
        "test_deg = test_graph.in_degrees(\n",
        "            range(test_graph.number_of_nodes())).float().view(-1,1)\n",
        "test_node_id = torch.arange(0, num_nodes, dtype=torch.long).view(-1, 1)\n",
        "test_rel = torch.from_numpy(test_rel).view(-1, 1)\n",
        "test_norm = torch.from_numpy(test_norm).view(-1, 1)\n",
        "test_graph.ndata.update({'id': test_node_id, 'norm': test_norm})\n",
        "test_graph.edata['type'] = test_rel\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "# build adj list and calculate degrees for sampling\n",
        "adj_list, degrees = get_adj_and_degrees(num_nodes, train_data)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "model_state_file = 'model_state.pth'\n",
        "forward_time = []\n",
        "backward_time = []\n",
        "\n",
        "# training loop\n",
        "print(\"start training...\")\n",
        "\n",
        "epoch = 0\n",
        "best_mrr = 0\n",
        "while True:\n",
        "    model.train()\n",
        "    epoch += 1\n",
        "\n",
        "    # perform edge neighborhood sampling to generate training graph and data\n",
        "    g, node_id, edge_type, node_norm, data, labels = \\\n",
        "        generate_sampled_graph_and_labels(\n",
        "            train_data, args['graph_batch_size'], args['graph_split_size'],\n",
        "            num_rels, adj_list, degrees, args['negative_sample'])\n",
        "    print(\"Done edge sampling\")\n",
        "\n",
        "    # set node/edge feature\n",
        "    node_id = torch.from_numpy(node_id).view(-1, 1)\n",
        "    edge_type = torch.from_numpy(edge_type).view(-1, 1)\n",
        "    node_norm = torch.from_numpy(node_norm).view(-1, 1)\n",
        "    data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n",
        "    deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1)\n",
        "    if use_cuda:\n",
        "        node_id, deg = node_id.cuda(), deg.cuda()\n",
        "        edge_type, node_norm = edge_type.cuda(), node_norm.cuda()\n",
        "        data, labels = data.cuda(), labels.cuda()\n",
        "    g.ndata.update({'id': node_id, 'norm': node_norm})\n",
        "    g.edata['type'] = edge_type\n",
        "\n",
        "    t0 = time.time()\n",
        "    loss = model.get_loss(g, data, labels)\n",
        "    t1 = time.time()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), args['grad_norm']) # clip gradients\n",
        "    optimizer.step()\n",
        "    t2 = time.time()\n",
        "\n",
        "    forward_time.append(t1 - t0)\n",
        "    backward_time.append(t2 - t1)\n",
        "    print(\"Epoch {:04d} | Loss {:.4f} | Best MRR {:.4f} | Forward {:.4f}s | Backward {:.4f}s\".\n",
        "          format(epoch, loss.item(), best_mrr, forward_time[-1], backward_time[-1]))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # validation\n",
        "    if epoch % args['evaluate_every'] == 0:\n",
        "        # perform validation on CPU because full graph is too large\n",
        "        if use_cuda:\n",
        "            model.cpu()\n",
        "        model.eval()\n",
        "        print(\"start eval\")\n",
        "        mrr = evaluate(test_graph, model, valid_data, num_nodes,\n",
        "                             hits=[1, 3, 10], eval_bz=args['eval_batch_size'])\n",
        "        # save best model\n",
        "        if mrr < best_mrr:\n",
        "            if epoch >= args['n_epochs']:\n",
        "                break\n",
        "        else:\n",
        "            best_mrr = mrr\n",
        "            torch.save({'state_dict': model.state_dict(), 'epoch': epoch},\n",
        "                       model_state_file)\n",
        "        if use_cuda:\n",
        "            model.cuda()\n",
        "\n",
        "print(\"training done\")\n",
        "print(\"Mean forward time: {:4f}s\".format(np.mean(forward_time)))\n",
        "print(\"Mean Backward time: {:4f}s\".format(np.mean(backward_time)))\n",
        "\n",
        "print(\"\\nstart testing:\")\n",
        "# use best model checkpoint\n",
        "checkpoint = torch.load(model_state_file)\n",
        "if use_cuda:\n",
        "    model.cpu() # test on CPU\n",
        "model.eval()\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "print(\"Using best epoch: {}\".format(checkpoint['epoch']))\n",
        "evaluate(test_graph, model, test_data, num_nodes, hits=[1, 3, 10],\n",
        "               eval_bz=args['eval_batch_size'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}